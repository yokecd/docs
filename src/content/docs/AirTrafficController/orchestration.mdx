---
title: Orchestration
---

At its core, the AirTrafficController (ATC) was conceived to enable the deployment of packages as custom resources in your cluster.

Beyond this, it includes a set of primitive features that together enable flexible and robust resource orchestration:

- Dynamic Mode  
- Cluster Access  
- Resource Access Matchers  
- Custom Resource Status Updates  

Classic Kubernetes application management tools have offered little support for orchestration.  
Here, we define *orchestration* as the ability to express relationships between resources in a release or package, specifically in terms of order, coordination, and state.

The reason for this gap is historical: packaging tools like **Helm** and **Kustomize** are client-side manifest generators.  
They encourage the idea that Kubernetes applications are simply flat lists of YAML manifests.  
Even server-side deployment tools like **ArgoCD** or **FluxCD** are limited in this regard, as they share the same worldview and ultimately deploy packages through these formats.

> **Note**  
> Tools like Helm and ArgoCD support limited forms of orchestration, for example, Helm pre/post-install hooks or ArgoCD sync waves.  
> However, these approaches are shallow: they don’t persist throughout the lifetime of the application.

Traditionally, if you needed intelligent, reactive, and orchestrated application deployment strategies, the solution was to build a custom operator.  
While valid, this approach comes with technical challenges, as well as development and maintenance costs that not every team or organization is ready to take on.

Yoke does not suffer from this issue as it operates with a completely different model, one where applications are defined by executable code. It lets you focus on *application orchestration* by writing a program that:  

1. Reads the input (the desired state of the custom resource).
2. Reads live state from the cluster.  
3. Updates the resource’s status.  
4. Emits the desired resources to be applied to the cluster.

As the system evolves, statuses update, jobs complete, deployments become ready, secrets change, and so on, your program is automatically re-evaluated.  
This allows you to orchestrate your package’s lifecycle, synchronize data, and react dynamically to the state of the cluster.


## Examples

The following examples are written in Go to take advantage of the Kubernetes WASI SDK provided by the yoke project to lookup cluster state.

> If an SDK does not exist in your preferred WASM compatible language, please open an issue to help track its development.

#### A Job Pipeline

This example represents a custom resource called `Pipeline`. Instances of this resource trigger multiple jobs to be done sequentially and the state of the current job is reported back to the pipeline resource.

```go
package main

import (
  "encoding/json"
  "fmt"
  "os"
  "slices"

  batchv1 "k8s.io/api/batch/v1"
  metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
  "k8s.io/apimachinery/pkg/util/yaml"

  "github.com/yokecd/yoke/pkg/flight"
  "github.com/yokecd/yoke/pkg/flight/wasi/k8s"
)

type Pipeline struct {
  metav1.TypeMeta
  metav1.ObjectMeta `json:"metadata"`
  Spec              struct {
    // ... Your fields ...
  } `json:"spec,omitzero"`
  Status struct {
    // Your status fields.
    // We will use a simple msg for our example but it can be whatever you wish it to be.
    Msg string `json:"msg"`
  }
}

func main() {
  if err := run(); err != nil {
    fmt.Fprintln(os.Stderr, err)
    os.Exit(1)
  }
}

func run() error {
  var pipeline Pipeline
  if err := yaml.NewYAMLToJSONDecoder(os.Stdin).Decode(&pipeline); err != nil {
    return fmt.Errorf("failed to decode stdin into pipeline: %w", err)
  }

  resources := flight.Resources{&pipeline}

  for _, job := range []*batchv1.Job{
    {
      TypeMeta:   metav1.TypeMeta{APIVersion: batchv1.SchemeGroupVersion.Identifier(), Kind: "Job"},
      ObjectMeta: metav1.ObjectMeta{Name: pipeline.Name + "-one"},
      Spec:       batchv1.JobSpec{
        // TODO
      },
    },
    {
      TypeMeta:   metav1.TypeMeta{APIVersion: batchv1.SchemeGroupVersion.Identifier(), Kind: "Job"},
      ObjectMeta: metav1.ObjectMeta{Name: pipeline.Name + "-two"},
      Spec:       batchv1.JobSpec{
        // TODO
      },
    },
    {
      TypeMeta:   metav1.TypeMeta{APIVersion: batchv1.SchemeGroupVersion.Identifier(), Kind: "Job"},
      ObjectMeta: metav1.ObjectMeta{Name: pipeline.Name + "-three"},
      Spec:       batchv1.JobSpec{
        // TODO
      },
    },
  } {
    // add the current job to the desired package state.
    resources = append(resources, job)

    // Check if the current job has completed.
    // If not write the state as is to stdout and wait for the next job update to trigger re-evaluation.
    ok, err := hasJobCompleted(&pipeline, job)
    if err != nil {
      return fmt.Errorf("failed to wait for job alpha to complete: %w", err)
    }
    if !ok {
      return json.NewEncoder(os.Stdout).Encode(resources)
    }
  }

  pipeline.Status.Msg = "all jobs have completed"

  return json.NewEncoder(os.Stdout).Encode(resources)
}

func isJobStatus(job *batchv1.Job, typ batchv1.JobConditionType) bool {
  return job != nil && slices.ContainsFunc(job.Status.Conditions, func(condition batchv1.JobCondition) bool {
    return condition.Type == typ
  })
}

func hasJobCompleted(pipeline *Pipeline, job *batchv1.Job) (ok bool, err error) {
  live, err := k8s.LookupResource(job)
  if err != nil && !k8s.IsErrNotFound(err) {
    return false, fmt.Errorf("failed to lookup job %s: %w", job.Name, err)
  }
  if isJobStatus(live, batchv1.JobFailed) {
    pipeline.Status.Msg = fmt.Sprintf("job %s failed", job.Name)
    return false, nil
  }
  if !isJobStatus(live, batchv1.JobComplete) {
    pipeline.Status.Msg = fmt.Sprintf("waiting for job %s to complete", job.Name)
    return false, nil
  }
  return true, nil
}
```
